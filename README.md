# Data Engineer

### <img align="center" src="assets/mortarboard.png" alt="santhoshguntupalli" height="50" width="60" /> Education
Master's in Management Information Systems

### <img align="center" src="assets/tool-box.png" alt="santhoshguntupalli" height="50" width="60" /> Skillset
## Programming Languages/Frameworks <img align="center" src="assets/programming.png" alt="santhoshguntupalli" height="30" width="40" />:
Python, Java, JavaScript, C, C#, C++, Scala, R, Shell Scripting, Golang

### Cloud Technologies <img align="center" src="assets/cloud.png" alt="santhoshguntupalli" height="30" width="40" />:
AWS, GCP, Azure, Snowflake, Oracle, Docker, Kubernetes

### Big Data and Data Engineering Tools/Services <img align="center" src="assets/big-data.png" alt="santhoshguntupalli" height="30" width="40" />:
Spark, Kafka, Hadoop, Hive, Airflow, HBase, Nifi, Teradata, Amazon RedShift, MapReduce, Flume, Flink, Informatica, Talend, AWS Glue, Amazon S3, Databricks, Azure Data Factory (ADF), Synapse Analytics, Trifacta, JSON, Avro, Parquet, ORC, XML, Protobuf, ELK Stack, PostgreSQL, MongoDB, Google BigQuery, Elasticsearch, HDFS, Metastore

### Machine Learning<img align="center" src="assets/machine-learning.png" alt="santhoshguntupalli" height="30" width="40" />:
TensorFlow, PyTorch, scikit-learn, PySpark, NLTK, LLM‚Äôs 

### DevOps, Monitoring, and Other Tools/Services <img align="center" src="assets/tool-box.png" alt="santhoshguntupalli" height="30" width="40" />:
Jenkins, JIRA, Confluence, Tableau, Power BI, GitHub, Git, RESTful, Splunk, Prometheus, PowerShell, Linux, UI/UX, Bash, Pub/Sub, Jupyter Notebooks, PyCharm.</strong></p>

### <img align="center" src="assets/project-management.png" alt="santhoshguntupalli" height="50" width="60" /> Projects

# GhostWriter Teams
<img align="center" src="Assets/MRS Sample.png" alt="santhoshguntupalli" height="200" width="400" />
Ghostwriter Teams is an AI-powered collaborative writing assistant that simulates a creative marketing team using local large language models (via Ollama) and LangChain agents. Built with Streamlit, the app lets users brainstorm, critique, and generate content calendars with the help of five specialized AI agents:

üé® Zara ‚Äì Creative strategist (hooks, themes, story arcs)
‚úçÔ∏è Max ‚Äì Content writer (posts, threads, articles)
üìä Mira ‚Äì Research analyst (insights, trends, timing)
üîç Eva ‚Äì Critique expert (feedback, scores, rewrites)
üõ°Ô∏è Leo ‚Äì Brand guardian (tone, consistency, alignment)

üí° Key Features
Full marketing content generation for multiple platforms
Editable, exportable calendars with strategic agent feedback
Multilingual output and local-first processing (no API keys needed)
Interactive UI for selecting agents, downloading kits, and comparing campaigns
PDF, CSV, and Markdown export options for campaigns
# [Movie Recommendation Engine]( https://mrs-sg-bfc2e6fa78db.herokuapp.com/)

<img align="center" src="assets/MRS Sample.png" alt="santhoshguntupalli" height="200" width="400" />

Created a Movie Recommendation System that suggests films based on a chosen movie selection.

Skills: Machine Learning, Natural Language Processing (NLP), Python Programming, Feature Engineering, Recommendation Systems. Data Pre-processing, Database 
Management Systems, API Integration, Model Evaluation, Data Visualisation

Tools: Scikit-learn, TensorFlow, NLTK, Pandas, NumPy, TF-IDF Vectorizer, Jupyter Notebooks, PyCharm, Flask, GitHub

# [Data Visualization projects]( https://public.tableau.com/app/profile/santhosh.guntupalli/vizzes )

Produced diverse data visualizations across my professional tenure and academic endeavors.

Skills: Data Analysis, Visualization Design, Statistical Knowledge, Storytelling with Data, Critical Thinking, Domain Knowledge

Tools: Matplotlib, Tableau, Pandas, Python, SQL, NoSQL, Git, GitHub,

# [Real time Stock Market Analysis]( https://github.com/guntupalli09/stock_market-real_time-analysis )

Real-time stock market analysis providing live trend indicators for informed investment decisions

Skills: Data Streaming, Cloud Services, Data Processing, Data Warehousing, API Integration, Model Evaluation

Tools: Kafka, Amazon EC2, Amazon S3, Crawler, AWS Glue Datalog, Amazon Athena, GitHub, jupyter notebook

# [Snake Game With Python](assets/PythonSnakeGame.gif)

<img align="center" src="assets/PythonSnakeGame.gif" alt="santhoshguntupalli" height="200" width="400" />

Created a dynamic Snake Game using Tkinter, showcasing skills in GUI programming and event handling. Excels in problem-solving, logical thinking, and attention to detail

# [Pong Game- AI--using Python and Neat](assets/PythonPongGame-AI.gif)

<img align="center" src="assets/PythonPongGame-AI.gif" alt="santhoshguntupalli" height="200" width="400" />

Utilizing the NEAT (NeuroEvolution of Augmenting Topologies) algorithm, an AI model is trained to play Pong in Python. NEAT evolves neural networks, enhancing their structure and weights over generations. The trained AI learns to play Pong autonomously through reinforcement learning, optimizing its performance to maximize gameplay proficiency.


### <img align="center" src="assets/certificate.png" alt="santhoshguntupalli" height="50" width="60" /> Certifications
1. META Database Structures and Management with MySQL
2. META Version Control
3. META Advanced MySQL
4. Prompt Engineering For Chatgpt - Vanderblit University
5. IBM Exploratory Data Analysis for Machine Learning
6. Machine Learning Specialisation - University of Washington

## <img align="center" src="assets/career.png" alt="santhoshguntupalli" height="50" width="60" /> Work Experience



### FedEx Dataworks, Remote                                                                                                                     June 2024 - Present
Position: Data Engineer II

‚Ä¢	Developed and implemented a strategic cost calculator tool using Databricks, ADF, Event Hubs, Docker, Kubernetes, and ADLS Gen 2, achieving significant annual savings and offering leadership insights for cost optimization and efficiency.
‚Ä¢	ETL pipeline development in ADF, using Azure Events for real-time data triggers. Managed and structured metadata with Unity Catalog, ensuring scalable, accurate data ingestion and transformation processes to support strategic decision-making.
‚Ä¢	Utilized PySpark, Scala, and Advanced SQL for large-scale data processing and analysis in Databricks, producing accurate, actionable cost insights that influenced critical business decisions.
‚Ä¢	Integrated and optimized multiple data sources for cohesive analysis, working closely with UI/UX and API teams to establish real-time access, enhancing data accessibility for leadership and cross-functional teams.
‚Ä¢	Automated critical cost metrics for fuel burn, landing fees, and block time calculations using Python in Databricks. 
‚Ä¢	Provided detailed data analytics for geographic segmentation, utilizing Unity Catalog and Power BI to present region-specific trends and support informed, data-driven decision-making for business growth.
‚Ä¢	Leveraged expertise in multi-cloud environments (Azure and GCP) to establish streamlined workflows and interoperability, enabling high-performance data operations that support ongoing analysis and forecasting.
‚Ä¢	Strengthened cross-functional collaboration by working with development, UI/UX, DevOps, and data science teams to ensure seamless integration and deployment cycles.

### Vedhops It Services,Remote                                                                                                                  March 2024- june 2024
Position: Data Engineer

‚Ä¢	Improved computational efficiency by 20% through optimization strategies in distributed and cloud environments.
‚Ä¢	Deployed and maintained 3 cloud-based data engineering solutions, contributing to a 15% increase in data product usability.
‚Ä¢	Led large-scale data engineering projects using Azure Databricks, Azure Data Factory, Azure SQL D, and Azure Synapse Analytics, achieving a 25% improvement in deployment efficiency.
‚Ä¢	Implemented Azure Databricks workflows, reducing data processing time by 30% and enhancing overall system reliability.
‚Ä¢	Developed and operationalized capabilities for near real-time high-volume streaming, achieving a 40% reduction in data processing latency.
‚Ä¢	Designed and deployed large-scale technical solutions, resulting in a 30% improvement in overall system performance.
‚Ä¢	Implemented automated testing frameworks, reducing software defects by 15% and improving overall system reliability.
‚Ä¢	Spearheaded the migration of legacy systems to cloud-native architectures, achieving a 30% improvement in scalability and resource utilization.
‚Ä¢	Proficient in leveraging AWS services, including EC2, S3, and AWS Lambda, for scalable and cost-effective cloud infrastructure.

### LTI Mindtree, Hyderabad, India                                                     September 2019- August 2022
Position: Data Engineer

‚Ä¢	Designed and implemented robust ETL pipelines using Azure Data Factory and AWS Glue to aggregate data from multiple sources, ensuring efficient data transformation and loading into Azure SQL Data Warehouse and Amazon Redshift.
‚Ä¢	Optimized data workflows within Azure SQL Data Warehouse and Amazon Redshift, leveraging Databricks for data transformation and analysis, achieving a 30% reduction in processing time and enabling more timely insights for business decision-making
‚Ä¢	Conducted data analysis, investigation, and lineage studies, leading to a 15% enhancement in data quality and access.
‚Ä¢	Demonstrated expertise in Spark, Kafka, and other high-volume data tools, achieving a 20% increase in data processing efficiency.
‚Ä¢	Utilized SQL and NoSQL storage tools, achieving a 15% improvement in database query performance.
‚Ä¢	Worked with different data types (JSON, XML, Parquet, Avro) for both batch and streaming ingestions, achieving a 25% reduction in data processing errors.
‚Ä¢	Developed and implemented alerting and monitoring frameworks, Infrastructure as Code (IaC) through Terraform, and data integration through APIs and/or REST services, contributing to a 20% increase in system reliability. 
‚Ä¢	Enhanced data security measures through the implementation of encryption protocols, resulting in a 25% reduction in security incidents.



