# Data Engineer

###  Education
Master's in Management Information Systems

## Skillset
### Programming Languages/Frameworks:
SQL, Python, Java, JavaScript, Scala, R, Pyspark

### Cloud Technologies :
Azure, Amazon Web Services, Google Cloud Platform, Snowflake, Docker, Kubernetes

### Big Data and Data Engineering Tools/Services :
Spark, Kafka, Hadoop, Hive, Airflow, Nifi, Teradata, Amazon RedShift, MapReduce, Flume, Flink, Informatica, Talend, ELK Stack, Google BigQuery, Elasticsearch, Metastore

### Machine Learning:
TensorFlow, PyTorch, scikit-learn, PySpark, NLTK, LLMâ€™s 

### Data Visualisation
Power BI, Tableau, Looker, Qlik

### DevOps, Monitoring:
Jenkins, JIRA, Confluence, GitHub, Git

###  Projects

# [Movie Recommendation Engine]( https://mrs-sg-bfc2e6fa78db.herokuapp.com/)

<img align="center" src="assets/MRS Sample.png" alt="santhoshguntupalli" height="200" width="400" />

Created a Movie Recommendation System that suggests films based on a chosen movie selection.

Skills: Machine Learning, Natural Language Processing (NLP), Python Programming, Feature Engineering, Recommendation Systems. Data Pre-processing, Database 
Management Systems, API Integration, Model Evaluation, Data Visualisation

Tools: Scikit-learn, TensorFlow, NLTK, Pandas, NumPy, TF-IDF Vectorizer, Jupyter Notebooks, PyCharm, Flask, GitHub

# [Data Visualization projects]( https://public.tableau.com/app/profile/santhosh.guntupalli/vizzes )

Produced diverse data visualizations across my professional tenure and academic endeavors.

Skills: Data Analysis, Visualization Design, Statistical Knowledge, Storytelling with Data, Critical Thinking, Domain Knowledge

Tools: Matplotlib, Tableau, Pandas, Python, SQL, NoSQL, Git, GitHub,

# [Real time Stock Market Analysis]( https://github.com/guntupalli09/stock_market-real_time-analysis )

Real-time stock market analysis providing live trend indicators for informed investment decisions

Skills: Data Streaming, Cloud Services, Data Processing, Data Warehousing, API Integration, Model Evaluation

Tools: Kafka, Amazon EC2, Amazon S3, Crawler, AWS Glue Datalog, Amazon Athena, GitHub, jupyter notebook

# [Snake Game With Python](assets/PythonSnakeGame.gif)

<img align="center" src="assets/PythonSnakeGame.gif" alt="santhoshguntupalli" height="200" width="400" />

Created a dynamic Snake Game using Tkinter, showcasing skills in GUI programming and event handling. Excels in problem-solving, logical thinking, and attention to detail

# [Pong Game- AI--using Python and Neat](assets/PythonPongGame-AI.gif)

<img align="center" src="assets/PythonPongGame-AI.gif" alt="santhoshguntupalli" height="200" width="400" />

Utilizing the NEAT (NeuroEvolution of Augmenting Topologies) algorithm, an AI model is trained to play Pong in Python. NEAT evolves neural networks, enhancing their structure and weights over generations. The trained AI learns to play Pong autonomously through reinforcement learning, optimizing its performance to maximize gameplay proficiency.


### Certifications
1. META Database Structures and Management with MySQL
2. META Version Control
3. META Advanced MySQL
4. Prompt Engineering For Chatgpt - Vanderblit University
5. IBM Exploratory Data Analysis for Machine Learning
6. Machine Learning Specialisation - University of Washington

## Work Experience
### FedEx Dataworks| Remote                                                                                                            [May 2024 - Present]
#### Position: Data Engineer II

Developed and implemented a strategic cost optimization tool using Databricks, ADF, Event Hubs, Docker, Kubernetes, and ADLS Gen 2, driving significant annual savings and providing leadership insights for cost optimization and efficiency.

Led comprehensive ETL pipeline development in ADF, using Azure Events for real-time data triggers, and managed and structured metadata with Unity Catalog, ensuring scalable, accurate data ingestion and transformation processes that support strategic decision-making.

Utilized PySpark, Scala, and Advanced SQL for large-scale data processing and analysis in Databricks, producing accurate, actionable insights that influenced critical business decisions and supported operational efficiency.

Integrated and optimized multiple data sources for cohesive analysis, utilizing Power BI and Unity Catalog for detailed reporting and geographic segmentation, enhancing data accessibility and enabling informed, data-driven decision-making across Azure and GCP environments.

### CVS Health | Dallas, Tx                                                                                                              [March 2024 - June 2024]
#### Position: Business Intelligence Analyst 
Leveraged data analytics to drive improvements in patient outcomes, streamline operations, and support strategic decision-making. Collected and integrated data from multiple sources, including electronic health records (EHR), patient management systems, and financial databases. Performed complex queries and data aggregation, while using Python for in-depth analysis and statistical modeling. Developed interactive dashboards and reports using Power BI, Tableau, and Looker, providing stakeholders with actionable insights on key performance indicators, such as patient wait times, treatment effectiveness, and resource allocation.

Responsible for ensuring data compliance with healthcare regulations, such as HIPAA, to maintain data security and patient privacy. I implemented data governance practices to ensure data accuracy and accessibility, which involved regular audits and collaboration with cross-functional teams. I also provided ongoing performance monitoring and reporting, analyzing trends to offer insights that informed clinical decision-making and improved care quality. Working closely with clinical, administrative, and IT teams, I delivered reports that were crucial for compliance and operational efficiencies.

I worked extensively within cloud environments like Azure, AWS, and Google Cloud to manage vast healthcare datasets and enable cross-functional collaboration. Built data warehouses, applied machine learning models for predictive analytics, and supported compliance efforts. Transformed complex data into impactful recommendations, enhancing both patient care and operational efficiencies.

### LTI Mindtree| Hyderabad                                                                                                                [September 2019- August 2022]
#### Position: Data Engineer

Managed 100 terabytes of diverse data utilizing Apache Spark, Kafka, and Hadoop, achieving a 35% increase in data processing efficiency.
Loaded 50 petabytes into Amazon Redshift and Google BigQuery, elevating analytics and reporting speed by 40%.

Collaborated with 15 teams to optimize data models, resulting in a 25% boost in retrieval speed and a 20% reduction in storage expenses.
Attained a 30% increase in query performance via SQL fine-tuning, indexing, and partitioning strategies.

Automated 50 data workflows, reducing manual effort by 30% and enhancing pipeline monitoring efficiency.
Oversaw 10 data warehousing solutions, increasing analytics speed by 50%, while handling 20 data streams for real-time processing, reducing processing time by 40%. Additionally, leveraged Python, R, and machine learning to develop 30 predictive models, achieving a 15% improvement in prediction accuracy.




