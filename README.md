# Data Engineer

###  Education
Master's in Management Information Systems

### Skillset
## Programming Languages/Frameworks :
SQL, Python, Java, JavaScript, Scala, R, Pyspark

### Cloud Technologies :
Azure, Amazon Web Services, Google Cloud Platform, Snowflake, Docker, Kubernetes

### Big Data and Data Engineering Tools/Services :
Spark, Kafka, Hadoop, Hive, Airflow, HBase, Nifi, Teradata, Amazon RedShift, MapReduce, Flume, Flink, Informatica, Talend, AWS Glue, Amazon S3, Databricks, Azure Data Factory (ADF), Synapse Analytics, Trifacta, JSON, Avro, Parquet, ORC, XML, Protobuf, ELK Stack, Google BigQuery, Elasticsearch, HDFS, Metastore

### Machine Learning:
TensorFlow, PyTorch, scikit-learn, PySpark, NLTK, LLMâ€™s 

### DevOps, Monitoring, and Other Tools/Services:
Jenkins, JIRA, Confluence, Tableau, Power BI, GitHub, Git, RESTful, Splunk, Prometheus, PowerShell UI/UX, Bash, Pub/Sub, , PyCharm.</strong></p>

###  Projects

# [Movie Recommendation Engine]( https://mrs-sg-bfc2e6fa78db.herokuapp.com/)

<img align="center" src="assets/MRS Sample.png" alt="santhoshguntupalli" height="200" width="400" />

Created a Movie Recommendation System that suggests films based on a chosen movie selection.

Skills: Machine Learning, Natural Language Processing (NLP), Python Programming, Feature Engineering, Recommendation Systems. Data Pre-processing, Database 
Management Systems, API Integration, Model Evaluation, Data Visualisation

Tools: Scikit-learn, TensorFlow, NLTK, Pandas, NumPy, TF-IDF Vectorizer, Jupyter Notebooks, PyCharm, Flask, GitHub

# [Data Visualization projects]( https://public.tableau.com/app/profile/santhosh.guntupalli/vizzes )

Produced diverse data visualizations across my professional tenure and academic endeavors.

Skills: Data Analysis, Visualization Design, Statistical Knowledge, Storytelling with Data, Critical Thinking, Domain Knowledge

Tools: Matplotlib, Tableau, Pandas, Python, SQL, NoSQL, Git, GitHub,

# [Real time Stock Market Analysis]( https://github.com/guntupalli09/stock_market-real_time-analysis )

Real-time stock market analysis providing live trend indicators for informed investment decisions

Skills: Data Streaming, Cloud Services, Data Processing, Data Warehousing, API Integration, Model Evaluation

Tools: Kafka, Amazon EC2, Amazon S3, Crawler, AWS Glue Datalog, Amazon Athena, GitHub, jupyter notebook

# [Snake Game With Python](assets/PythonSnakeGame.gif)

<img align="center" src="assets/PythonSnakeGame.gif" alt="santhoshguntupalli" height="200" width="400" />

Created a dynamic Snake Game using Tkinter, showcasing skills in GUI programming and event handling. Excels in problem-solving, logical thinking, and attention to detail

# [Pong Game- AI--using Python and Neat](assets/PythonPongGame-AI.gif)

<img align="center" src="assets/PythonPongGame-AI.gif" alt="santhoshguntupalli" height="200" width="400" />

Utilizing the NEAT (NeuroEvolution of Augmenting Topologies) algorithm, an AI model is trained to play Pong in Python. NEAT evolves neural networks, enhancing their structure and weights over generations. The trained AI learns to play Pong autonomously through reinforcement learning, optimizing its performance to maximize gameplay proficiency.


### Certifications
1. META Database Structures and Management with MySQL
2. META Version Control
3. META Advanced MySQL
4. Prompt Engineering For Chatgpt - Vanderblit University
5. IBM Exploratory Data Analysis for Machine Learning
6. Machine Learning Specialisation - University of Washington

## Work Experience
### FedEx Dataworks, Remote                                                                                                            [May 2024 - Present]
#### Position: Data Engineer II

Developed and implemented a strategic cost optimization tool using Databricks, ADF, Event Hubs, Docker, Kubernetes, and ADLS Gen 2, driving significant annual savings and providing leadership insights for cost optimization and efficiency.

Led comprehensive ETL pipeline development in ADF, using Azure Events for real-time data triggers, and managed and structured metadata with Unity Catalog, ensuring scalable, accurate data ingestion and transformation processes that support strategic decision-making.

Utilized PySpark, Scala, and Advanced SQL for large-scale data processing and analysis in Databricks, producing accurate, actionable insights that influenced critical business decisions and supported operational efficiency.

Integrated and optimized multiple data sources for cohesive analysis, utilizing Power BI and Unity Catalog for detailed reporting and geographic segmentation, enhancing data accessibility and enabling informed, data-driven decision-making across Azure and GCP environments.


### LTI Mindtree, Hyderabad                                                                                                                                 [September 2019- August 2022]
#### Position: Data Engineer

Managed 100 terabytes of diverse data utilizing Apache Spark, Kafka, and Hadoop, achieving a 35% increase in data processing efficiency.
Loaded 50 petabytes into Amazon Redshift and Google BigQuery, elevating analytics and reporting speed by 40%.

Collaborated with 15 teams to optimize data models, resulting in a 25% boost in retrieval speed and a 20% reduction in storage expenses.
Attained a 30% increase in query performance via SQL fine-tuning, indexing, and partitioning strategies.

Automated 50 data workflows, reducing manual effort by 30% and enhancing pipeline monitoring efficiency.
Oversaw 10 data warehousing solutions, increasing analytics speed by 50%, while handling 20 data streams for real-time processing, reducing processing time by 40%. Additionally, leveraged Python, R, and machine learning to develop 30 predictive models, achieving a 15% improvement in prediction accuracy.




